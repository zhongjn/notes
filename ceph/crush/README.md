# Ceph 中的分片摆放算法 CRUSH

## 背景

在一些分布式系统中（如分布式缓存），我们常常使用一致性哈希来进行数据分片。集群中的任何一台机器、集群外的客户端机器，在具备*集群视图*（集群中所有机器的信息）的情况下，可以计算出任意数据对应的分片位置，不需要中心化的元数据节点介入。

在分布式存储系统中，情况有所不同。因为一个分片需要存在于若干个*副本*中，以保证机器错误时数据不丢失。为了避免相互关联的错误（如一整个机架断电）导致多副本丢失，各副本需要摆放在几个物理隔离的错误域（如不同机架、不同可用区）之中。一致性哈希中难以建模错误域，不能胜任分布式存储中分片的任务。

因此，在常见的分布式存储系统中，我们需要中心化的元数据节点，来负责分片的副本摆放。集群中的机器不再有能力自行计算分片位置，而需要向元数据节点查询。中心化的副本选取可能会导致一些可扩展性问题。

Ceph 是一个为消灭了中心化的元数据节点而设计的分布式存储系统，集群中的机器需要自行根据集群视图来计算出分片位置。Ceph 需要一个分片算法来代替一致性哈希，不丢失一致性哈希优点的同时，又加入错误域等存储需要的建模——这就是我们今天的主角 CRUSH。它具有以下特点：

1. 继承了一致性哈希的优点，如集群变动时数据迁移量小、查询高效等。
2. 加入了对错误域的建模，允许不同尺度上的副本物理隔离。
3. 加入了逐机器的权重，允许异质的机器集群。
4. 加入了过载保护，数据分布更加平衡，避免单机过载。

本文将把 CRUSH 算法从 Ceph 的上下文中抽取出来，作为一个通用的分片算法讲解。本文期望读者对一致性哈希、分布式存储系统有一定的了解。

注：本文基于 Ceph 作者2007年的博士学位论文[TODO:ref]，不代表当前 CRUSH 算法的实现。本人水平有限，如有错误烦请各位指正。

## 集群视图

*集群视图*（cluster map）是 CRUSH 要用到的一个重要数据，它包含整个集群下的所有服务器，将集群以_树状_结构表示。树中的各个中间层可自定义，但通常对应各个尺度的错误域，即集群的物理组织（如可用区、机柜行、机柜等）。树的叶子层即为实际存储机器。

以下图片展示了一个包含根、机柜行、机柜、单机四层的集群视图。

[TODO:pic]

中间节点可以具备不同的*种类*（bucket type）。不同种类的节点具有不同的行为、特性。用户根据节点所处层级特性来进行权衡，选择种类。节点种类决定了节点的以下特性：

- 在子节点变化时，子节点间的数据迁移量。
- 查询效率（某份数据属于哪个子节点？）。
- 是否允许赋予子节点权重。

CRUSH 提供了4种节点种类。







